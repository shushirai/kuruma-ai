import cv2
import torch
from torchvision import models, transforms
from PIL import Image
import os
from ultralytics import YOLO

# âœ… è»Šç¨®æƒ…å ±
car_prices = {
    "Toyota Prius": {"æ–°è»Šä¾¡æ ¼": "275ä¸‡å††ã€œ460ä¸‡å††", "ä¸­å¤ä¾¡æ ¼": "3.7ä¸‡å††ã€œ569ä¸‡å††"},
    "Honda Fit": {"æ–°è»Šä¾¡æ ¼": "172ä¸‡å††ã€œ250.7ä¸‡å††", "ä¸­å¤ä¾¡æ ¼": "4.8ä¸‡å††ã€œ290ä¸‡å††"},
    "Nissan Note": {"æ–°è»Šä¾¡æ ¼": "229.9ä¸‡å††ã€œ306.4ä¸‡å††", "ä¸­å¤ä¾¡æ ¼": "5ä¸‡å††ã€œ295ä¸‡å††"},
}
class_names = list(car_prices.keys())

# âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰é–¢æ•°
def load_classifier(model_path="./models/best_model.pth", num_classes=len(class_names)):
    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)
    model.load_state_dict(torch.load(model_path, map_location="cpu"))
    model.eval()
    return model

# âœ… ãƒ‡ãƒã‚¤ã‚¹è¨­å®š (CUDAå„ªå…ˆ)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ–¥ï¸ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

# âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
classifier = load_classifier().to(device)
detector = YOLO("yolov8n.pt")  # è»½é‡ç‰ˆYOLO

# âœ… å‰å‡¦ç†
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# âœ… å‹•ç”»æ¨è«–ï¼‹åˆ†é¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
def predict_video_with_detection(input_video_path, output_video_path="output_with_detection.mp4", threshold=0.6):
    cap = cv2.VideoCapture(input_video_path)
    if not cap.isOpened():
        print("âŒ å‹•ç”»ã‚’é–‹ã‘ã¾ã›ã‚“")
        return

    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

    frame_idx = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        results = detector.predict(source=frame, stream=True, verbose=False)

        for result in results:
            boxes = result.boxes.xyxy.cpu().numpy()
            labels = result.boxes.cls.cpu().numpy()
            scores = result.boxes.conf.cpu().numpy()

            for box, label, score in zip(boxes, labels, scores):
                if int(label) in [2, 5, 7]:  # car, bus, truck
                    x1, y1, x2, y2 = map(int, box)
                    cropped_car = frame[y1:y2, x1:x2]

                    if cropped_car.size == 0:
                        continue

                    img_pil = Image.fromarray(cv2.cvtColor(cropped_car, cv2.COLOR_BGR2RGB)).convert("RGB")
                    input_tensor = transform(img_pil).unsqueeze(0).to(device)
                    with torch.no_grad():
                        outputs = classifier(input_tensor)
                        probs = torch.nn.functional.softmax(outputs, dim=1)[0]
                        top_prob, top_idx = torch.topk(probs, k=1)

                        confidence = top_prob.item()
                        car_label = class_names[top_idx.item()]

                    # æç”»ãƒ†ã‚­ã‚¹ãƒˆ
                    if confidence < threshold:
                        text = f"åˆ¤å®šä¸èƒ½ ({confidence:.2f})"
                    else:
                        text = f"{car_label} ({confidence:.2f})"

                    # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ï¼‹ãƒ©ãƒ™ãƒ«
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(frame, text, (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        out.write(frame)

        frame_idx += 1
        if frame_idx % 30 == 0:
            print(f"âœ… {frame_idx} ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ä¸­...")

    cap.release()
    out.release()
    print(f"ğŸ¯ æ¨è«–å®Œäº†: {output_video_path}")

# âœ… ä½¿ç”¨ä¾‹ï¼ˆæœ‰åŠ¹ã«ã—ã¦å®Ÿè¡Œï¼‰
predict_video_with_detection("test1.mp4", "output_with_detection.mp4")
